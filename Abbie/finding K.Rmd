---
title: "Finding best K for KNN"
author: "Abbie Hayward"
date: "2023-11-05"
output:
  rmdformats::readthedown:
    code_folding: hide
    fig_width: 8
    fig_height: 8
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## Introduction and set-up

For this project the aim of our report is to create models to perform binary classifications on a flight delay dataset, and to perform a model comparison by a specified rubric.
The data we use for the entire project is available at: 
 "https://www.kaggle.com/datasets/threnjen/2019-airline-delays-and-cancellations".
 
 

```{r}
#packages required for report 
install_packages = function(install = FALSE) {
  if (install == TRUE) {
    packages = c("knitr","tidyverse", "class", "readr", "pROC", "dplyr","fastDummies", "tidyr", "caret", "FNN")

    
    for (p in packages) { install.packages(p) }
  }
}

#To install packages required set (install = TRUE), you only need to do this once
install_packages(install = FALSE)
library("knitr") 
library("class") # For knn
library("pROC") # For ROCs
library("readr")
library("tidyr")
library("dplyr")
library("tidyverse")
library("fastDummies")
library("caret")
```

```{r}
#setwd("~/Data Science Toolbox")
```

From previous exploratory data analysis (EDA), the flight dataset has been split into X/y, 80:20 train:test sets. Within the EDA, we found a large class imbalance from those flights that were defined as "delayed" (late by 15 minutes) being dominated by those not delayed. To account for the imbalance we found SMOTE to be one of the best performing techniques at creating a balanced dataset. **Input something about smote**


```{r}
X_normal_train = read.csv("X_train.csv")
y_normal_train = read.csv("y_train.csv")
X_smote_train = read.csv("X_smote_train.csv")
y_smote_train = read.csv("y_smote_train.csv")
X_normal_test = read.csv("X_test.csv")
y_normal_test = read.csv("y_test.csv")
X_smote_test = read.csv("X_smote_test.csv")
y_smote_test=read.csv("y_smote_test.csv")

```

Now all the datsets are downloaded, since K-NN is a distance based algorithm, it is important that I "scale" my data. To keep my data within [0,1] although this doesn't handle outliers as well as standardisation, normalization is useful when you don't know the distribution of data, or the model you're implimenting does not use any underlying assumptions about distribution, like K-NN [1]. It's also useful when data has varying scales, which affect an algorithm like K-NN drastically where larger features dominate the distance calculations. 

The method I used to create my "scaled" data set, was the min-max normalisation with the `preProcess()` function. [2]

[2] (https://www.geeksforgeeks.org/how-to-normalize-and-standardize-data-in-r/)
[1] https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff

To determine which hyperparameter K is "better" for the K-NN classification, I am going to run a downsampled 10-fold cross-validation method to find which K has the better accuracy on the test sets for the normal data and SMOTE.
Downsampling is used to reduce processing speed as K-NN is a **computationally intense** algorithm, with long implementation time, and since this is only for tuning the parameters we don't need all data to train and test the model.

## K evaluation on original dataset


```{r}
#combining X_train and y_train to trained dataset 
set.seed(123)
train_data <- cbind(y_normal_train, X_normal_train)
#dim(train_data)

sample_train = sample_n(train_data,40000)

#using the min-max mehtod to normalise the data
process <- preProcess(as.data.frame(sample_train), method=c("range"))
norm_train_df <- predict(process, as.data.frame(sample_train))

#checking dimensions
#dim(norm_train_df)

X_norm_sample = select(norm_train_df, -DEP_DEL15)
y_norm_sample = norm_train_df$DEP_DEL15
```

The same is done for the SMOTE dataframe...
```{r}
set.seed(123)
#combining X_train and y_train to trained dataset 
train_smote <- cbind(y_smote_train, X_smote_train)
dim(train_smote)

sample_smote = (sample_n(train_data, 40000))

#Normalising using min-max 
process <- preProcess(as.data.frame(sample_smote), method=c("range"))
norm_smote <- predict(process, as.data.frame(sample_smote))
dim(norm_smote)

X_smote_norm = select(norm_smote, -DEP_DEL15)
y_smote_norm = norm_smote$DEP_DEL15
```

Now with downsampled training data, I run a 10-fold cross validation over 10 different values of K to find a sutable size for the hyperparamter K, with normal or standardised data. Note this is for the original training dataset (without smote).


```{r}
set.seed(123) #setting seed for reproducible data


#K-fold Cross-validation
n = nrow(norm_train_df)
k_values <- seq(1,sqrt(n),30)
K <- 10
sets <- sample( rep(1:K,n)[1:n], n)

test_error_knn <- rep(0,length(k_values))
for (k in 1:K)
{
test_set <- which(sets==k)
train_set <- (1:n)[-test_set]
X_train <- X_norm_sample[train_set,]
X_test <- X_norm_sample[test_set,]
y_train <- y_norm_sample[train_set]
y_test <- y_norm_sample[test_set]

# For each k.value fit knn classifier and find test error
for (i in 1:length(k_values)) {
y_test_pred <- knn(train=X_train,
test=X_test,
cl=y_train,
k=k_values[i])
test_error_knn[i] <- test_error_knn[i] + sum(y_test_pred!=y_test)
}
}
test_error_knn <- test_error_knn/n


plot(k_values,
test_error_knn,
type="b",
col = "blue",
main="Test errors for KNN")

```
Here there is an obvious elbow at K=31, however since the scale is so large its hard to tell the optimal k and which scale is best for the data. So I perform the KNN-test-error again on a smaller subset of K from (1,30) to find the best k
```{r}
set.seed(123) #setting seed for reproducible data
#K-fold Cross-validation
n = nrow(norm_train_df)
k_values <- seq(1,31,4)
K <- 10
sets <- sample( rep(1:K,n)[1:n], n)

test_error_knn <- rep(0,length(k_values))
for (k in 1:K)
{
test_set <- which(sets==k)
train_set <- (1:n)[-test_set]
X_train <- X_norm_sample[train_set,]
X_test <- X_norm_sample[test_set,]
y_train <- y_norm_sample[train_set]
y_test <- y_norm_sample[test_set]

# For each k.value fit knn classifier and find test error
for (i in 1:length(k_values)) {
y_test_pred <- knn(train=X_train,
test=X_test,
cl=y_train,
k=k_values[i])
test_error_knn[i] <- test_error_knn[i] + sum(y_test_pred!=y_test)
}
}
test_error_knn <- test_error_knn/n


library("data.table")
#Make table of results 
df = list( k_values = k_values,
              error_normalised = test_error_knn )
DT_2 <- data.table(do.call(cbind, df))    # Transform list to data.table
DT_2

```
From the table we can see that the smallest error for the normal undersampled data, is at K=21 with normalised data. Hence this will be used for model training and the ROC curve. 

 

```{r}
set.seed(123)
k_values <- seq(1,sqrt(n),30)

#K-fold Cross-validation
n = nrow(sample_smote)
K <- 10
sets <- sample( rep(1:K,n)[1:n], n)

test_error_knn_smote <- rep(0,length(k_values))
for (k in 1:K)
{
test_set <- which(sets==k)
train_set <- (1:n)[-test_set]
X_train <- X_smote_norm[train_set,]
X_test <- X_smote_norm[test_set,]
y_train <- y_smote_norm[train_set]
y_test <- y_smote_norm[test_set]

# For each k.value fit knn classifier and find test error
for (i in 1:length(k_values)) {
y_test_pred <- knn(train=X_train,
test=X_test,
cl=y_train,
k=k_values[i])
test_error_knn_smote[i] <- test_error_knn_smote[i] + sum(y_test_pred!=y_test)
}
}
test_error_knn_smote <- test_error_knn_smote/n

plot(k_values,
test_error_knn_smote,
type="b",
col = "blue",
main="Test errors for KNN smote")

```

```{r}
set.seed(123)
k_values <- seq(1,31,5)

#K-fold Cross-validation
n = nrow(sample_smote)
K <- 10
sets <- sample( rep(1:K,n)[1:n], n)

test_error_knn_smote1 <- rep(0,length(k_values))
for (k in 1:K)
{
test_set <- which(sets==k)
train_set <- (1:n)[-test_set]
X_train <- X_smote_norm[train_set,]
X_test <- X_smote_norm[test_set,]
y_train <- y_smote_norm[train_set]
y_test <- y_smote_norm[test_set]

# For each k.value fit knn classifier and find test error
for (i in 1:length(k_values)) {
y_test_pred <- knn(train=X_train,
test=X_test,
cl=y_train,
k=k_values[i])
test_error_knn_smote1[i] <- test_error_knn_smote1[i] + sum(y_test_pred!=y_test)
}
}
test_error_knn_smote1 <- test_error_knn_smote1/n

library("data.table")
#Make table of results 
df = list( k_values = k_values,
             error_normalised = test_error_knn_smote1 )
DT_2 <- data.table(do.call(cbind, df))    # Transform list to data.table
DT_2

```

As you can see from both tables of a subset of K, the K with the smallest test error is K=21, for both datasets scaled. This is what I will use when training my model to find accuracy and the ROC curve. 

## References 
[1]  https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff
[2] (https://www.geeksforgeeks.org/how-to-normalize-and-standardize-data-in-r/)












