---
title: "KNN training and testing"
author: "Abbie Hayward"
date: "2023-11-06"
output:
  rmdformats::html_clean:
    code_folding: hide
    fig_width: 8
    fig_height: 8
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r}
#packages required for report 
install_packages = function(install = FALSE) {
  if (install == TRUE) {
    packages = c("knitr","tidyverse", "class", "readr", "pROC", "dplyr","fastDummies", "tidyr", "caret", "Metrics")

    
    for (p in packages) { install.packages(p) }
  }
}

#To install packages required set (install = TRUE), you only need to do this once
install_packages(install = FALSE)
library("knitr") 
library("class") # For knn
library("pROC") # For ROCs
library("readr")
library("tidyr")
library("dplyr")
library("tidyverse")
library("fastDummies")
library("caret")
library("kableExtra")
library("Metrics")
```

```{r}
X_normal_train = read.csv("X_train.csv")
y_normal_train = read.csv("y_train.csv")
X_smote_train = read.csv("X_smote_train.csv")
y_smote_train = read.csv("y_smote_train.csv")
X_normal_test = read.csv("X_test.csv")
y_normal_test = read.csv("y_test.csv")
X_smote_test = read.csv("X_smote_test.csv")
y_smote_test=read.csv("y_smote_test.csv")

```

```{r}
#making full test and train dataset, scaled.
train = cbind(y_normal_train, X_normal_train)
test = cbind(y_normal_test, X_normal_test)
train_smote = cbind(y_smote_train, X_smote_train)

process <- preProcess(as.data.frame(train), method=c("range"))
norm_train <- predict(process, as.data.frame(train))

process <- preProcess(as.data.frame(test), method=c("range"))
norm_test <- predict(process, as.data.frame(test))

process <- preProcess(as.data.frame(train_smote), method=c("range"))
norm_smote_train <- predict(process, as.data.frame(train_smote))
 #note may need to run again to test on the original data due to synthetic dataframe
```



# Model development for KNN

From preliminary work for fine-tuning my model in the "finding K" rmd file, I found that for 10-fold cross validation an optimal K to look at would be K=21, on both standardised datasets for my original and smote.This is the rmd file `finding_k`.

The code implemented below is taken from DST block 05 workshop.
```{r}
learnmodel=function(modelclass,formula,train,test, 
                    predictfn=function(x,newdata)predict(x,newdata,type="response"),
                    ...){ 
  ## Start by sorting out the formula target
  if(class(formula)=="character") formula=formula(formula)
  y=as.character(formula[2])
  
  ## Now run the learning 
  model=modelclass(formula,data=train,...)
  
  ## Predict on training data
  trainpred0=predictfn(model,newdata=train)
  trainpred=ifelse(trainpred0 > 0.5,1,0)
  
  ## Predict on testing data
  testpred0=predictfn(model,newdata=test)
  testpred=ifelse(testpred0 > 0.5,1,0)
  
  ## Organise the data for return
  trainres=data.frame(truth=train[,y],pred=trainpred,pred0=trainpred0)
  testres=data.frame(truth=test[,y],pred=testpred,pred0=testpred0)
  
  ## Compute ROC
  testroc=roc(truth~pred0,data=as.data.frame(testres))
  list(model=model,
       trainres=trainres,
       testres=testres,
       train=train,
       test=test,
       roc=testroc)
}
```

## 1.2.2 KNN
```{r}
library("class")
knnclass=function(formula,data,k){
  ## knn does not provide the usual interface, so we define it from scratch here
  ## We need to know what the x and y parts of y~x are, and to store all the data and k
  if(class(formula)=="character") formula=formula(formula)
  y=as.character(formula[2])
  x=labels(terms(formula, data=data))
  ret=list(formula=formula,train=data,k=k,x=x,y=y)
  class(ret)="knnclass"
  ret
}
predict.knnclass=function(x,newdata,...){
  ## knn can now be run on the new data. It returns the results as a factor with attributes "pr" where the probability of that classification is made. So we have to transform this into a probability.
  predtmp=knn(x$train[,x$x], newdata[,x$x], x$train[,x$y], k = x$k, prob=TRUE)
  pred0=attr(predtmp,"pr")
  pred=as.numeric(predtmp)-1
  pred0[pred==0]= 1-pred0[pred==0
]
  pred0
}
```


Note that preliminary evaluation of my model, with hyperparameter tuning, and data transformation justification is performed in the `finding k` rmd file, and so this is mainly about storing a trained model to preform model evaluation and comparison in the results and comparison section of the report. There we explore the ROC curves more in depth, along with accuracy, precision, recall and area under the curve (AUC)  comparison. This section does not explore these at not-wanting to repeat information that would be better interpreted as a whole on a ipyn file. 


# Normal model
```{r}
set.seed(123)
knnmodel1=learnmodel(knnclass,DEP_DEL15~.,norm_train,norm_test,k=21,predictfn=predict.knnclass)
plot(knnmodel1$roc, main = "Unbalanced ROC")


```

```{r}

predicted_normal = as.data.frame(knnmodel1[["testres"]][["pred"]])

AUC = as.numeric(knnmodel1[["roc"]][["auc"]])
AUC
```
As we can see from the ROC curve it doens't deviate too far from the middle line which suggests model difficulty with classifying accurately. While the closeness of the curve to the line suggests weak predictive power, since ROC looks at threshold evaluation it could also suggest a lot of over-lapping of the classes within the data. The best way to evaluate the curve is to look at the area underneath (AUC), whist further analysis can still be made. 


```{r}
write.csv(predicted_normal,"C:/Users/abbie/OneDrive/Documents/Data Science Toolbox/predicted_normal.csv")

# sensitivity and specificity to plot false positive rate and true positive rate
sensitivity_normal = as.data.frame(knnmodel1[["roc"]][["sensitivities"]])
specificity_normal = as.data.frame(knnmodel1[["roc"]][["specificities"]])
sens_spec_norm = cbind(sensitivity_normal, specificity_normal)
write.csv(sens_spec_norm,"C:/Users/abbie/OneDrive/Documents/Data Science Toolbox/sens_spec_norm.csv")
```



# Smote model
```{r}
set.seed(123)
knnmodel_smote=learnmodel(knnclass,DEP_DEL15~.,norm_smote_train,norm_test,k=21,predictfn=predict.knnclass)
plot(knnmodel_smote$roc, main = "Smote ROC")

```


```{r}

predicted_smote = as.data.frame(knnmodel_smote[["testres"]][["pred"]])

AUC_smote = as.numeric(knnmodel_smote[["roc"]][["auc"]])
AUC_smote

```
When comparing the height of the ROC curves and the AUC you can tell that the SMOTE dataset has increased in model predictabilitiy acucracy, over all threshold, i.e. better average class separability. While this can indicate a better model it is important to note that if there's a large disparity between cost of a false positive vs a false negative (i.e incorrect predictions). In the case of predicting flight delays for an insurance company it may be better to prioritise false positives over false negatives, as it may be used in  the insurance premium decision [1]. 




```{r}
write.csv(predicted_smote,"C:/Users/abbie/OneDrive/Documents/Data Science Toolbox/predicted_smote.csv")
#sensitivity and specificity to plot false positive rate and true positive rate
sensitivity_smote = as.data.frame(knnmodel_smote[["roc"]][["sensitivities"]])
specificity_smote = as.data.frame(knnmodel_smote[["roc"]][["specificities"]])
sens_spec_smote = cbind(sensitivity_smote, specificity_smote)
write.csv(sens_spec_smote,"C:/Users/abbie/OneDrive/Documents/Data Science Toolbox/sens_spec_smote.csv")
```

```{r}


pred_probs_knn_original = as.data.frame(knnmodel1[["testres"]][["pred0"]])
pred_probs_knn_smote =  as.data.frame(knnmodel_smote[["testres"]][["pred0"]])
write.csv(pred_probs_knn_original,"C:/Users/abbie/OneDrive/Documents/Data Science Toolbox/pred_probs_knn_original.csv")
write.csv(pred_probs_knn_smote,"C:/Users/abbie/OneDrive/Documents/Data Science Toolbox/pred_probs_knn_smote.csv")
```

# References 
[1] https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc




